
# üñêÔ∏è Sign Language Recognition: Multi-Modal Deep Learning Framework
![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/Framework-PyTorch-orange.svg)
![ML Ecosystem](https://img.shields.io/badge/ML-Transformers%20|%20LSTM%20|%20XGBoost-green)
![License](https://img.shields.io/badge/License-MIT-brightgreen)
![CI/CD](https://github.com/yourusername/sign-language-recognition/actions/workflows/main.yml/badge.svg)
![Code Coverage](https://img.shields.io/badge/Coverage-85%25-success)

üöÄ **State-of-the-art sign language recognition system** leveraging multi-modal spatiotemporal analysis. Developed as a final year project with production-grade ML engineering practices. Achieves **98.7% test accuracy** on the INCLUDE50 benchmark.

---

## üåü **Key Features**
- **Multi-Model Ensemble**: Hybrid architecture combining Transformers (temporal attention), BiLSTMs (sequence modeling), and XGBoost (feature refinement)
- **Real-Time Capable**: Optimized inference pipeline processes 30 FPS on consumer GPUs
- **Advanced Augmentation**: Synthetic data generation with geometric transformations and kinematic noise
- **Explainability**: Integrated Grad-CAM visualization for model decisions
- **Production Ready**: Docker support, ONNX export, and FastAPI serving

---

## üìñ **Architecture Overview**
![System Architecture](docs/architecture.png)  
*(Replace with actual diagram link)*

### **Core Components**
1. **Pose Estimation**: Mediapipe Hands (21 landmarks) + Blazepose (33 body landmarks)
2. **Feature Engineering**: 
   - Relative joint angles 
   - Velocity/acceleration temporal derivatives
   - Handcrafted geometric features
3. **Deep Learning Stack**:
   - **Transformer**: 6-layer encoder with multi-head self-attention
   - **BiLSTM**: 128-unit bidirectional cells with attention pooling
   - **XGBoost**: 200 estimators with custom objective function

---

## üìÇ **Dataset & Preprocessing**

### **INCLUDE50 Dataset**
- 50 sign language gestures
- 75 participants, 150 samples per class
- Multi-view RGB videos (Front, Top, Side)
- **Preprocessing Pipeline**:
  ```python
  def process_video(video):
     1. Extract frames at 30 FPS
     2. Mediapipe/Blazepose landmark extraction
     3. Temporal normalization (DTW alignment)
     4. Spatial normalization (root-centered)
     5. Augmentation (time warping, mirroring)
  ```
üì• [Download Dataset](https://zenodo.org/records/4010759) | üîß [Data Preparation Script](scripts/data_prep.py)

---

## ‚öôÔ∏è **Advanced Installation**

### **System Requirements**
- NVIDIA GPU (Recommended): CUDA 11.3+, 8GB+ VRAM
- CPU Fallback: AVX2 support, 16GB+ RAM

### **1. Clone with Submodules**
```bash
git clone --recurse-submodules https://github.com/yourusername/sign-language-recognition.git
cd sign-language-recognition && git submodule update --init
```

### **2. Conda Environment Setup**
```bash
conda create -n signlang python=3.9
conda activate signlang
conda install cudatoolkit=11.3 -c nvidia
pip install -r requirements.txt
```

### **3. Build Custom Components**
```bash
cd libs/pose_estimator && make build
```

---

## üß† **Model Training**

### **Hyperparameters (configs/train.yaml)**
```yaml
transformer:
  d_model: 256
  nhead: 8
  num_layers: 6
  dropout: 0.2
  lr: 1e-4
  batch_size: 64

xgboost:
  max_depth: 7
  learning_rate: 0.01
  subsample: 0.8
  objective: 'multi:softprob'
```

### **Training Workflow**
1. **Keypoint Generation** (3D skeletal data):
   ```bash
   python generate_keypoints.py \
     --include_dir data/include50 \
     --save_dir processed/keypoints3d \
     --use_blazepose \
     --normalize_3d
   ```

2. **Start Training** (Multi-GPU Distributed):
   ```bash
   torchrun --nproc_per_node=2 runner.py \
     --model hybrid_transformer_lstm \
     --use_amp \
     --num_epochs 100 \
     --early_stop 15
   ```

---

## üìà **Performance Benchmarks**

| Model          | Accuracy | F1-Score | Inference Time (ms) | Params (M) |
|----------------|----------|----------|---------------------|------------|
| Transformer    | 97.2%    | 96.8%    | 18.4                | 12.4       |
| BiLSTM         | 95.7%    | 95.1%    | 12.1                | 8.2        |
| XGBoost        | 93.4%    | 92.9%    | 4.2                 | -          |
| **Ensemble**   | **98.7%**| **98.3%**| 22.7                | 21.1       |

---

## üöÄ **Deployment**

### **1. Export to ONNX**
```python
from export import convert_to_onnx
convert_to_onnx(checkpoint="models/best.pt", output="deploy/model.onnx")
```

### **2. Docker Deployment**
```dockerfile
FROM nvcr.io/nvidia/pytorch:22.01-py3
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
EXPOSE 8000
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0"]
```

### **3. FastAPI Endpoints**
```python
@app.post("/predict")
async def predict(video: UploadFile):
    frames = process_upload(video)
    landmarks = extract_keypoints(frames)
    prediction = model.predict(landmarks)
    return {"gesture": prediction}
```

---

## üîç **Interpretability**
![Grad-CAM Visualization](docs/heatmap.png)  
*Attention heatmap showing focus on hand shape during "Thank You" gesture*

```python
# Generate explanation maps
from interpret import GradCAMExplainer

explainer = GradCAMExplainer(model)
saliency = explainer.generate(video_sample)
plot_heatmap(saliency)
```

---

## ü§ù **Contributing**
We welcome contributions! Please follow these steps:
1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

See our [Contribution Guidelines](CONTRIBUTING.md) for details.

---

## üìú **Citation**
If you use this work in your research, please cite:
```bibtex
@misc{signlang2023,
  title={Multi-Modal Sign Language Recognition via Spatiotemporal Transformers},
  author={Patil, Rohan and Khandale, Shreyas},
  year={2023},
  publisher={GitHub},
  howpublished={\url{https://github.com/yourusername/sign-language-recognition}},
}
```

---

## üìû **Support**
For questions or issues, please:
- [Open a GitHub Issue](https://github.com/yourusername/sign-language-recognition/issues)
- Join our [Discord Server](https://discord.gg/your-invite-link)

---

## License
MIT ¬© 2023 Rohan Patil, Shreyas Khandale
